{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One basic Mnist TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linbinyang/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/linbinyang/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/linbinyang/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/linbinyang/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/linbinyang/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/linbinyang/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/linbinyang/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/linbinyang/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/linbinyang/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/linbinyang/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/linbinyang/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/linbinyang/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-b29b0e83318f>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/linbinyang/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/linbinyang/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./dataset/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/linbinyang/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./dataset/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/linbinyang/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./dataset/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./dataset/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/linbinyang/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = read_data_sets(\"./dataset\", one_hot=True, reshape=False, validation_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_DIR = 'new_ckpt_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.001\n",
    "        self.global_step = tf.Variable(0,trainable = False)\n",
    "        self.x = tf.placeholder(tf.float32, [None, 28, 28, 1], name = \"x_input\")\n",
    "        self.label = tf.placeholder(tf.float32,[None,10], name = 'y_input')\n",
    "        self.w = tf.Variable(tf.zeros([784,10]))\n",
    "        self.b = tf.Variable(tf.zeros([10]))\n",
    "        self.XX = tf.reshape(self.x, shape=[-1, 784])\n",
    "        self.y = tf.nn.softmax(tf.matmul(self.XX,self.w) + self.b, name = 'z_output')\n",
    "        self.loss = -tf.reduce_mean(self.label * tf.log(self.y) + 1e-10)\n",
    "        self.train = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss,global_step = self.global_step)\n",
    "        predict = tf.equal(tf.argmax(self.label,1),tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(predict,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.net = Network()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train(self):\n",
    "        batch_size = 64\n",
    "        train_step = 10000\n",
    "        step = 0\n",
    "        saver = tf.train.Saver(max_to_keep = 0)\n",
    "        ckpt = tf.train.get_checkpoint_state(CKPT_DIR)\n",
    "        if ckpt and ckpt.get_checkpoint_state(CKPT_DIR):\n",
    "            saver.restore(self.sess,ckpt.model_checkpoint_path)\n",
    "            step = self.sess.run(self.net.global_step) # Fetch the global step from the CKPT model\n",
    "            print('continue from')\n",
    "            print('  -> Minibatch update : ',step)\n",
    "        while step < train_step:\n",
    "            x, label = mnist.train.next_batch(batch_size)\n",
    "            _,loss = self.sess.run([self.net.train,self.net.loss], feed_dict = {self.net.x: x,self.net.label:label})\n",
    "            step = self.sess.run(self.net.global_step)\n",
    "            if step % 1000 == 0:\n",
    "                print('No %6d step, current loss: %.3f'%(step,loss))\n",
    "            if step % 10000 == 0:\n",
    "                saver.save(self.sess,CKPT_DIR + '/model',global_step = step)\n",
    "    \n",
    "    def calculate_accuracy(self):\n",
    "        test_x = mnist.test.images\n",
    "        test_label = mnist.test.labels\n",
    "        acc = self.sess.run(self.net.accuracy,feed_dict = {self.net.x:test_x,self.net.label:test_label})\n",
    "        print(\"acc: %.3f, Number of pic test: %d \" % (acc, len(test_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No   1000 step, current loss: 0.219\n",
      "No   2000 step, current loss: 0.209\n",
      "No   3000 step, current loss: 0.202\n",
      "No   4000 step, current loss: 0.194\n",
      "No   5000 step, current loss: 0.183\n",
      "No   6000 step, current loss: 0.183\n",
      "No   7000 step, current loss: 0.170\n",
      "No   8000 step, current loss: 0.170\n",
      "No   9000 step, current loss: 0.154\n",
      "No  10000 step, current loss: 0.157\n",
      "INFO:tensorflow:new_ckpt_dir/model-10000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "acc: 0.791, Number of pic test: 10000 \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    model = Train()\n",
    "    model.train()\n",
    "    model.calculate_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x, label = mnist.train.next_batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, new_label = mnist.train.next_batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print (x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_restore_inference():\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.import_meta_graph('./new_ckpt_dir/model-10000.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./new_ckpt_dir'))\n",
    "    input_x = sess.graph.get_tensor_by_name('x_input:0')\n",
    "    logits = sess.graph.get_tensor_by_name('z_output:0')\n",
    "    ret = sess.run(logits, {input_x:y})\n",
    "    print (ret)\n",
    "    print(np.argmax(ret, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./new_ckpt_dir/model-10000\n",
      "[[0.10841706 0.04617622 0.05912366 0.11703159 0.08475072 0.15665607\n",
      "  0.04944148 0.09974264 0.1467772  0.13188341]\n",
      " [0.22704913 0.03240506 0.09899905 0.22519769 0.05501195 0.11319795\n",
      "  0.07596913 0.03990334 0.08369131 0.04857536]\n",
      " [0.09220583 0.1146492  0.11611874 0.10832658 0.08583894 0.10964114\n",
      "  0.09676941 0.08767551 0.11509439 0.07368019]\n",
      " [0.05658335 0.05745472 0.08811751 0.07307006 0.18997613 0.08323205\n",
      "  0.11314528 0.09622805 0.11054733 0.13164555]\n",
      " [0.04586288 0.16799837 0.09570821 0.08433018 0.10872564 0.07173033\n",
      "  0.08054798 0.12296626 0.11433949 0.10779069]\n",
      " [0.21212715 0.03820959 0.10356621 0.08255032 0.06378555 0.06918816\n",
      "  0.2260393  0.05961361 0.08509041 0.05982979]\n",
      " [0.04118611 0.3221302  0.10235254 0.08764888 0.05758281 0.06247237\n",
      "  0.07284125 0.07626836 0.10535688 0.07216051]\n",
      " [0.14679338 0.02333928 0.08228805 0.040509   0.1910505  0.05809385\n",
      "  0.11407567 0.11011762 0.07561462 0.15811794]\n",
      " [0.24816258 0.05104037 0.06861065 0.12009436 0.05295517 0.12990846\n",
      "  0.08749465 0.04753845 0.13132276 0.06287251]\n",
      " [0.03563767 0.03808145 0.12265395 0.04941317 0.14879777 0.05228399\n",
      "  0.2641385  0.07634673 0.10213389 0.11051286]\n",
      " [0.15669599 0.04094879 0.0665334  0.10229594 0.07768019 0.12279803\n",
      "  0.2158907  0.03369837 0.10928289 0.07417566]\n",
      " [0.07862205 0.04456973 0.06476232 0.09545678 0.09868725 0.09273747\n",
      "  0.08031522 0.2095222  0.075382   0.15994492]\n",
      " [0.07488281 0.09173127 0.13243745 0.06919604 0.09691944 0.09623945\n",
      "  0.20139723 0.04868156 0.11357485 0.07493983]\n",
      " [0.06054089 0.24989228 0.08732463 0.09303417 0.06199872 0.08351427\n",
      "  0.09209245 0.08389519 0.10715406 0.0805533 ]\n",
      " [0.1655722  0.04204706 0.10994808 0.06494085 0.14215344 0.07458355\n",
      "  0.14861462 0.06442909 0.0839383  0.10377277]\n",
      " [0.07680776 0.05124157 0.1048215  0.11829099 0.06945647 0.08717897\n",
      "  0.0816975  0.11361714 0.2033713  0.09351689]\n",
      " [0.05563983 0.05658741 0.08926228 0.3017855  0.04811542 0.09665119\n",
      "  0.1018706  0.07514441 0.1046193  0.07032409]\n",
      " [0.04080079 0.04942138 0.05204575 0.05902098 0.07610343 0.04602382\n",
      "  0.03498304 0.4137973  0.07770868 0.15009482]\n",
      " [0.10056821 0.07889672 0.16989748 0.15316887 0.06733505 0.0853522\n",
      "  0.08394638 0.05619751 0.14698984 0.05764776]\n",
      " [0.04715311 0.19376948 0.14909472 0.09401485 0.08457807 0.06753467\n",
      "  0.06428789 0.06181852 0.16853575 0.06921289]\n",
      " [0.16987851 0.06637403 0.06733203 0.10100454 0.08734611 0.09185974\n",
      "  0.09578182 0.13848592 0.0892809  0.09265637]\n",
      " [0.15336977 0.03307699 0.06122744 0.20438437 0.05639979 0.15029569\n",
      "  0.07698639 0.05592687 0.12795608 0.08037657]\n",
      " [0.0452254  0.12682627 0.2198047  0.10202365 0.07771574 0.05571804\n",
      "  0.10164466 0.0791712  0.1172637  0.07460675]\n",
      " [0.04883747 0.07686849 0.11600266 0.15691976 0.0816335  0.08101998\n",
      "  0.13385977 0.07135152 0.16136988 0.07213705]\n",
      " [0.06657185 0.17248791 0.15981935 0.10988299 0.06983588 0.08044193\n",
      "  0.06546655 0.04857486 0.16629417 0.06062449]\n",
      " [0.1725225  0.05160449 0.06800593 0.10865092 0.07756309 0.17308618\n",
      "  0.08001925 0.07253804 0.10996483 0.08604481]\n",
      " [0.05645731 0.09333851 0.13623524 0.15079628 0.05152376 0.07939511\n",
      "  0.1154672  0.07297862 0.16346908 0.08033895]\n",
      " [0.10048102 0.0398753  0.22320198 0.07844862 0.10785835 0.06668706\n",
      "  0.10545398 0.06089945 0.12722878 0.08986544]\n",
      " [0.07338521 0.05472635 0.09838574 0.18098223 0.06855647 0.10294424\n",
      "  0.06291919 0.05974725 0.21415712 0.0841962 ]\n",
      " [0.18626456 0.0511132  0.0805822  0.11983258 0.07469875 0.12032703\n",
      "  0.08586328 0.08007438 0.11570498 0.08553898]\n",
      " [0.07356512 0.07503571 0.07594471 0.23846914 0.079094   0.11225623\n",
      "  0.09119553 0.06339962 0.10853302 0.08250682]\n",
      " [0.05314146 0.25105742 0.11543873 0.0994475  0.07155529 0.07111964\n",
      "  0.06568117 0.07219198 0.1288927  0.07147408]]\n",
      "[5 0 2 4 1 6 1 4 0 6 6 7 6 1 0 8 3 7 2 1 0 3 2 8 1 5 8 2 8 0 3 1]\n"
     ]
    }
   ],
   "source": [
    "model_restore_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "data = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = {}\n",
    "unit['first'] = y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open (\"data1.json\", \"w\") as fp:\n",
    "    fp.write(json.dumps(unit, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to reload model from the saved CPKT files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tensorboard to visualize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net_structure = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "wirter = tf.summary.FileWriter('logs/',sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $ tensorboard --logdir='/home/linbin/Tensorflow_Inference_C/logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we could use tensorboard to see the net structure from our web broswer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to reload CKPT Model from local and do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_filenames(model_dir, model_exp):\n",
    "    files = os.listdir(model_dir)\n",
    "    meta_files = [s for s in files if s.endswith('.meta')]\n",
    "    if len(meta_files)==0:\n",
    "        raise load_modelValueError('No meta file found in the model directory (%s)' % model_dir)\n",
    "    #fetch the goal meta file\n",
    "    goal_meta_file = meta_files[0]\n",
    "    for each_file in meta_files:\n",
    "        if each_file.startswith(model_exp):\n",
    "            goal_meta_file = each_file\n",
    "            break\n",
    "    ckpt = tf.train.get_checkpoint_state(model_dir)     # 通过checkpoint文件找到模型文件名\n",
    "    print (ckpt.model_checkpoint_path)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        # ckpt.model_checkpoint_path表示模型存储的位置，不需要提供模型的名字，它回去查看checkpoint文件\n",
    "        ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        return goal_meta_file, ckpt_file\n",
    "    \n",
    "    meta_files = [s for s in files if '.ckpt' in s]\n",
    "    max_step = -1\n",
    "    for f in files:\n",
    "        step_str = re.match(r'(^model-[\\w\\- ]+.ckpt-(\\d+))', f)\n",
    "        if step_str is not None and len(step_str.groups())>=2:\n",
    "            step = int(step_str.groups()[1])\n",
    "            if step > max_step:\n",
    "                max_step = step\n",
    "                ckpt_file = step_str.groups()[0]\n",
    "    return goal_meta_file, ckpt_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./new_ckpt_dir/model-10000\n"
     ]
    }
   ],
   "source": [
    "meta_file, ckpt_file = get_model_filenames(model_dir, model_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metagraph file: model-10000.meta\n",
      "Checkpoint file: model-10000\n"
     ]
    }
   ],
   "source": [
    "print ('Metagraph file: %s' % meta_file)\n",
    "print ('Checkpoint file: %s' % ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python import pywrap_tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tensor_name: ', 'Variable')\n",
      "('tensor_name: ', 'Variable_2')\n",
      "('tensor_name: ', 'Variable_1')\n"
     ]
    }
   ],
   "source": [
    "reader = pywrap_tensorflow.NewCheckpointReader(os.path.join(model_dir, ckpt_file))\n",
    "var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "for key in var_to_shape_map:\n",
    "    print(\"tensor_name: \", key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the tensor W in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./new_ckpt_dir/model-10000\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph(os.path.join(model_dir, meta_file))\n",
    "    saver.restore(tf.get_default_session(), os.path.join(model_dir, ckpt_file))\n",
    "    print (type(sess.run(tf.get_default_graph().get_tensor_by_name(\"Variable_1:0\"))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
